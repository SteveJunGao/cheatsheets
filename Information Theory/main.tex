% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=0.6in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, tabularx}
\usepackage{graphicx}
\usepackage{listings}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

 
% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\title{Information Theory}
\author{Jun Gao}
\date{August 31, 2018}

\maketitle
\section{Entropy}
\begin{definition}
	Entropy $H(X)$, which measures the uncertainty of a discrete random variable X, is defined as the $H(X)=\sum_i - p_i \log(p_i)$
\end{definition}
\textbf{Discussion: } Entropy is an effective measurement of uncertainty according to the Shanon's minimum description code theory. Suppose we want use a code to compress/encode the discrete random variable $X$ (each possible value $X_i$ should have a unique code) and the resulting code sequence is \textit{uniquely decodable} without referencing the whole sequence (\textit{prefix code, any code is not the prefix of other codes}), then the minimum code length is the entropy using Karft Inequality. 
\begin{theorem}
	\textbf{Karft Inequality:} For any prefix code, the codeword length $l_1,l_2,\cdots,l_m$ should satisfy the following inequality:
	\begin{eqnarray}
		\sum_i^m D^{-l_i} \le 1,
	\end{eqnarray} 
	and for any code length that satisfy such inequality, we could construct prefix code for this random variable with these code length.
\end{theorem}
\begin{proof}
	For prefix code, it equals to construct a tree with all the codeword being the leaves, without any codeword being the inner nodes. Then for a complete D-branch tree, each node $l_i$ should have $D^{l_{max}-l_i}$ descends, therefore $\sum_i D^{l_{max}-l_i} \le D^{l_{max}}$, thus $\sum_i^m D^{-l_i} \le 1$. Also given the code length, we could always construct this D-branch tree.
\end{proof}
Therefore, the minimum code length of a discrete random variable should satisfy Karft's inequality. Then we have the following optimization problem.
\begin{eqnarray}
	\min_{l_i} \sum_i p_il_i, s.t. \sum_i D^{-l_i}<1
\end{eqnarray}
To get such code, we first treat $l_i$ as continuous variable, using the Lagrange multiplier, it equals to the minimize of 
\begin{eqnarray}
	\min_{l_i,\lambda} \sum_i p_il_i + \lambda \sum_i D^{-l_i}
\end{eqnarray}
The derivative w.r.t. $l_i$ should satisfy $p_i-\lambda D^{-l_i}\ln D=0$, therefore $l_i=-\log_D p_i + \log_D (\lambda\ln D)$, we could also get the optimal $\lambda$ is $1/\ln D$, so the optimal $l_i$ is $-log_D p_i$. Extending to integer code length, we could proof $H(X) \le \mathbb{L}^* < H(X) + 1$, it's easy to proof this by just rounding to the $\lceil \rceil$ and do the summation.

\begin{definition}
	\textbf{Joint Entropy:} $H(X,Y)=-\sum_X\sum_Y p(x,y) \log p(x,y)$
\end{definition}
\begin{definition}
	\textbf{Conditional Entropy: $H(X|Y) = \sum_Y p(y) H(X|Y=y)=-\sum_Y\sum_X p(x,y)\log p(x|y)$}
\end{definition}

\begin{proposition}
	$H(X,Y)=H(Y) + H(X|Y)$, Prove this by just simple equation expansion.
\end{proposition}

\section{Relative Entropy}
\begin{definition}
	$D(p||q)=\sum_i p_i\log \frac{p_i}{q_i}$
\end{definition}
\textbf{Discussion:} Suppose the true distribution of a random variable is $p$, the minimum description length should be $H(p)$, but if we misuse another distribution $q$, the relative entropy measures how much extra bits we need to encode the random variable, which is $-\sum_i p_i\log q_i + \sum_i p_i \log p_i = \sum_i p_i\log \frac{p_i}{q_i} = D(p||q)$
\begin{proposition}
	$D(p||q)\ge 0s$
\end{proposition}
\begin{proof}
	Using Jenson Inequality, we have $\sum_i -p_i\log \frac{q_i}{p_i}\le\log\sum_i p_i\frac{q_i}{p_i}=0$
\end{proof}
\section{Mutual Information}
\begin{definition}
	$I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=\sum_X\sum_Yp(x,y)\log\frac{p(x,y)}{p(x)p(y)}=D(p(x,y)||p(x)p(y)$
\end{definition}
\end{document}
