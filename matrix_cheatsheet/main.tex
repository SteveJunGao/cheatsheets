% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, tabularx}
\usepackage{graphicx}
\usepackage{listings}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

 
% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\title{Matrix Theorem}
\author{Jun Gao}
\date{April 8, 2018}

\maketitle
\section{Determinant}
\begin{itemize}
\item det(AB) = det(A) * det(B)
\item det(I + A) = det(I + PVP) = det (I + V) = $\prod_i (1 + \lambda_i)$
\end{itemize}
 

\section{Rank}
\begin{theorem}
	Rank($AA^T$) = Rank(A)
\end{theorem}
\begin{proof}
	For any $x$ that makes $Ax=0$, we have $AA^Tx=0$. For any $x$ that makes $AA^Tx=0$, we have $x^TAA^Tx=0,$ so $Ax=0$
\end{proof}
\begin{theorem}
	Rank(AB) $\ge$ Rank(A) + Rank(B) - n.
\end{theorem}
\begin{proof}
	
\end{proof}
\section{SVD}
\subsection{EVD}
\begin{theorem}
	For any symmetric matrix M, there always exist eigenvalue value decomposition, such that $M=X\Gamma X^T$, and $XX^T=I_n$, $\Gamma=\text{diag}\{\gamma_1, \gamma_2,\cdots,\gamma_n\}$.
\end{theorem}
\begin{proof}
	
\end{proof}
\begin{theorem}
	For similar matrix ($A=P^{-1}BP$), they have same eigenvalues and eigenvectors
\end{theorem}
\begin{proof}
	from the deterimant analysis, the eigenvalue is determined by the detrimant poly-equation. then we have $|\lambda I - P^{-1}BP|=|P^{-1}(\lambda I-B)P|=|P^{-1}||\lambda I - B||P|=|\lambda I - B|.$
\end{proof}
\begin{theorem}
	$AA^T$ is symmetric semi positive definite.
\end{theorem}
\begin{proof}
	Consider $x^TAA^Tx=x^TA(x^TA)^T=||x^TA||^2=\lambda||x||^2.$ Therefore, $\lambda$ should be non-negative.
\end{proof}
\begin{theorem}
	For the solution in the least square approximation problem $\text{min}_x||Ax-B||^2$, where x is a vector. We have the closed form solution that: $x={(A^TA)}^{-1}A^TB$
\end{theorem}
\begin{proof}
	Calculate the first order derivative, we have $A^T(Ax-B)=0$, then $x={(A^TA)}^{-1}A^TB$. (We suppose A is column full rank here)
\end{proof}


\section{Lecture notes for Convex Optimization}
\begin{definition}
	Vector Space: the space is closed under addition and scale multiplication.
\end{definition}
\begin{definition}
	Norm: $||\cdot||$ denotes a mapping from $\mathbb{R}^n\to \mathbb{R}$ and should satisfy the following conditions:
	\begin{eqnarray}
		||a||&\ge& 0; \& ||x||=0 iff x=0 \\
		||ta|| &=& t||a||\\
		||\sum_i a_i|| &\le& \sum_i ||a_i||
	\end{eqnarray}
\end{definition}
{\bf Examples:} Euclidean Norm: $||x||_2 = (\sum_i(x_i^2))^{1/2}$; L-p norm $||x||_p=(\sum_i(x_i^p)^{1/p})$; L-infinite norm means the maximum value and L-0 norm means the number of non-zero values.

\begin{definition}
	$||\cdot||_2$ is special, as it's induced by an inner product. (inner product space)
\end{definition}
\begin{theorem}
	Cauchy-Schwatz inequality: 
	\begin{eqnarray}
		\langle x,y\rangle \le ||x||\cdot ||y||
	\end{eqnarray}
\end{theorem}
\begin{definition}
	Dual Norm $||\cdot||_*$: let $||\cdot||$ be a norm in $R^n$, the dual norm is defined as $||Z||_*=\sup_x\{Z^Tx| ||x||\le 1\}$
\end{definition}
{\bf Discussion:} 
\begin{itemize}
	\item The dual norm of L-2 norm $||\cdot||_2$ is also $||\cdot||_2$. 
	\item The dual norm of L-1 norm is L-$\infty$ norm. (Consider it as an expectation over $z_i$ and $x_i$ is the probability of $z_i$, so the maximum reached when we assign all the probability to $z_{max}$.)
	\item the dual norm of L-$\infty$ norm is L-1 norm.
	\item In general, the $||\cdot||_p$ norm, its dual norm is q-norm, where $1/p + 1/q = 1$
\end{itemize}

\begin{definition}
	Matrix Inner Product: $\langle X,Y\rangle = \sum\sum x_{ij}y_{ij}=tr(XY^T)$
\end{definition}
\begin{definition}
	Forbenius Norm $||X||_F = (\sum_i\sum_i x_{ij}^2)^{1/2}$
\end{definition}
\end{document}
