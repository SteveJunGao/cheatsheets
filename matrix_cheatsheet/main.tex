% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=0.75in]{geometry} 
\usepackage{amsmath,amsthm,amssymb, tabularx}
\usepackage{graphicx}
\usepackage{listings}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]

 
% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
 
\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\title{Matrix Theorem}
\author{Jun Gao}
\date{April 8, 2018}

\maketitle
\section{Determinant}
\begin{itemize}
\item det(AB) = det(A) * det(B)
\item det(I + A) = det(I + PVP) = det (I + V) = $\prod_i (1 + \lambda_i)$
\end{itemize}
 

\section{Rank}
\begin{theorem}
	Rank($AA^T$) = Rank(A)
\end{theorem}
\begin{proof}
	For any $x$ that makes $Ax=0$, we have $AA^Tx=0$. For any $x$ that makes $AA^Tx=0$, we have $x^TAA^Tx=0,$ so $Ax=0$
\end{proof}
\begin{theorem}
	Rank(AB) $\ge$ Rank(A) + Rank(B) - n.
\end{theorem}
\begin{proof}
	
\end{proof}
\section{SVD}
\subsection{EVD}
\begin{theorem}
	Jordan Theorem, for any square matrix $A$, there exists nonsingular matrix $T$, such that
	\begin{eqnarray}
		TAT^{-1} = diag\left[J_{m1}(\lambda_1),J_{m2}(\lambda_2),\cdots,J_{mk}(\lambda_k)\right],
	\end{eqnarray}
	where $J_{mi}(\lambda_i)$ is $mi\times mi$ matrix and equals:
	\[
	\begin{bmatrix}
		\lambda_i & 1 & 0 & \dots & 0\\
		0 & \lambda_i & 1& \\
		0 & \vdots & \vdots& \ddots&\vdots\\
			0 &0 & 0& \dots &1\\
		0 &0 & 0& \dots &\lambda_i\\
	\end{bmatrix}
	\]
\end{theorem}
\begin{theorem}
	For any symmetric matrix M, there always exist eigenvalue value decomposition, such that $M=X\Gamma X^T$, and $XX^T=I_n$, $\Gamma=\text{diag}\{\gamma_1, \gamma_2,\cdots,\gamma_n\}$.
\end{theorem}
\begin{proof}
	
\end{proof}
\begin{theorem}
	For similar matrix ($A=P^{-1}BP$), they have same eigenvalues and eigenvectors
\end{theorem}
\begin{proof}
	from the deterimant analysis, the eigenvalue is determined by the detrimant poly-equation. then we have $|\lambda I - P^{-1}BP|=|P^{-1}(\lambda I-B)P|=|P^{-1}||\lambda I - B||P|=|\lambda I - B|.$
\end{proof}
\begin{theorem}
	$AA^T$ is symmetric semi positive definite.
\end{theorem}
\begin{proof}
	Consider $x^TAA^Tx=x^TA(x^TA)^T=||x^TA||^2=\lambda||x||^2.$ Therefore, $\lambda$ should be non-negative.
\end{proof}
\begin{theorem}
	For the solution in the least square approximation problem $\text{min}_x||Ax-B||^2$, where x is a vector. We have the closed form solution that: $x={(A^TA)}^{-1}A^TB$
\end{theorem}
\begin{proof}
	Calculate the first order derivative, we have $A^T(Ax-B)=0$, then $x={(A^TA)}^{-1}A^TB$. (We suppose A is column full rank here)
\end{proof}


\section{Norm}
\subsection{Vector Norm}
\begin{definition}
	A function $f:R^n\to R$ is called norm if: a) nonnegative: $f(x)\ge 0,\forall x$; b) definite: $f(x)=0, iff. x=0$; c) homogeneous: $f(tx)=|t|f(x)$; d) triangle inequality: $f(x+y)\le f(x) + f(y)$.
\end{definition}
\begin{definition}
	In Euclidean space, the L-p norm is defined by $||x||_p = (\sum_i^n x_i^p)^{1/p}$
\end{definition}
\begin{definition}
	Dual Norm: Let $||\cdot||$ be the norm on $R^n$ vector space, the associated dual norm $||cdot||_*$ is defined as $||x||_*=\sup\{x^Tv \mid ||v||\le1 \}$.
\end{definition}

\subsection{Matrix Norm}
\begin{definition}
	Operator norm: Suppose $||\cdot||_a,||\cdot||_b$ are norms in $R^n, R^m$, respectively. the operator norm of matrix $X$ is defined by $||X||_{a,b}=\sup\{||Xv||_a \mid ||v||_b<1\}$.
\end{definition}
\begin{proposition}
	 the Operator norm induced by $L_2$ Euclidean Norm is spectral norm
\end{proposition}
\begin{proof}
	L2 Euclidean norm can be expressed as $||Xv||=\sqrt{v^TX^TXv}$. using SVD, we have $X=U\Sigma V^T, ||Xv||=\sqrt{v^TV\Sigma^2V^Tv}=||\Sigma V^Tv||$, therefore, the maximum achieves when $v$ is parallel with singular vector with max singular value.
\end{proof}
\begin{definition}
	Spectral Norm: maximum of singular values.
\end{definition}
\begin{definition}
	Nuclear Norm: sum of singular values
\end{definition}
\begin{definition}
	Forbenius norm: $||X||_F = (\sum_i\sum_j x_{ij}^2)^{1/2}$
\end{definition}

{\bf Discussion:} Relationship between the L0 norm and L1 norm: L1 norm is the smallest convex hull of the L0 norm. This is the key techniques to LASSO and Robust PCA, which use the nuclear norm as a surrogate the rank of a matrix. 

The calculation of nuclear norm? By the following equation, using SVD:
\[
tr(\sqrt{AA^T}) = tr(\sqrt{U\Sigma V^T (V\Sigma U^T)} = tr\sqrt{U\Sigma^2U^T} = \sum_i^n |\sigma_i|
\]
\begin{proposition}
	The dual norm of spectral norm is nuclear norm.
\end{proposition}
\begin{proof}
	Using the definition of dual norm, we need to prove:
	\[
	\sup_{\sigma(A)_1\le 1} tr(A^TX)=\sum_i\sigma(X)_i = tr(X^TX)^{1/2}
	\]
	We first prove $\sup_{\sigma(A)_1\le 1} tr(X^TA)\ge tr(X^TX)^{1/2}$. Let the $X=U\Sigma V^T$ by SVD. Then we have $tr((UIV^T)^TX)=tr(VU^TU\Sigma V^T)=tr(V\Sigma V^T)=tr(\Sigma)=\sum_i\sigma(X)_i$. Thus, $\sup_{\sigma(A)_1\le 1} tr(A^TX)\ge \sum_i\sigma(X)_i$. 
	
	For the other direction, we have: $\sup_{\sigma(A)_1\le 1} tr(A^TX) = \sup_{\sigma(A)_1\le 1} tr(A^TU\Sigma V^T)= \sup_{\sigma(A)_1\le 1} tr(V^TA^TU\Sigma) $ $= \sup_{\sigma(A)_1\le 1} \sum_{i=1}^n\sigma(X)_iv_i^TA^Tu_i\le \sup_{\sigma(A)_1\le 1}\sigma(X)_i\sigma(A)_1=\sum_{i=1}^n\sigma(X)_i$
\end{proof}
\begin{proposition}
	The nuclear norm is convex.
\end{proposition}
\begin{proof}
	content...
\end{proof}
\section{Lecture notes for Convex Optimization}
\begin{definition}
	Vector Space: the space is closed under addition and scale multiplication.
\end{definition}
\begin{definition}
	Norm: $||\cdot||$ denotes a mapping from $\mathbb{R}^n\to \mathbb{R}$ and should satisfy the following conditions:
	\begin{eqnarray}
		||a||&\ge& 0; \& ||x||=0 iff x=0 \\
		||ta|| &=& t||a||\\
		||\sum_i a_i|| &\le& \sum_i ||a_i||
	\end{eqnarray}
\end{definition}
{\bf Examples:} Euclidean Norm: $||x||_2 = (\sum_i(x_i^2))^{1/2}$; L-p norm $||x||_p=(\sum_i(x_i^p)^{1/p})$; L-infinite norm means the maximum value and L-0 norm means the number of non-zero values.

\begin{definition}
	$||\cdot||_2$ is special, as it's induced by an inner product. (inner product space)
\end{definition}
\begin{theorem}
	Cauchy-Schwatz inequality: 
	\begin{eqnarray}
		\langle x,y\rangle \le ||x||\cdot ||y||
	\end{eqnarray}
\end{theorem}
\begin{definition}
	Dual Norm $||\cdot||_*$: let $||\cdot||$ be a norm in $R^n$, the dual norm is defined as $||Z||_*=\sup_x\{Z^Tx| ||x||\le 1\}$
\end{definition}
{\bf Discussion:} 
\begin{itemize}
	\item The dual norm of L-2 norm $||\cdot||_2$ is also $||\cdot||_2$. 
	\item The dual norm of L-1 norm is L-$\infty$ norm. (Consider it as an expectation over $z_i$ and $x_i$ is the probability of $z_i$, so the maximum reached when we assign all the probability to $z_{max}$.)
	\item the dual norm of L-$\infty$ norm is L-1 norm.
	\item In general, the $||\cdot||_p$ norm, its dual norm is q-norm, where $1/p + 1/q = 1$
\end{itemize}

\begin{definition}
	Matrix Inner Product: $\langle X,Y\rangle = \sum\sum x_{ij}y_{ij}=tr(XY^T)$
\end{definition}
\begin{definition}
	Forbenius Norm $||X||_F = (\sum_i\sum_i x_{ij}^2)^{1/2}$
\end{definition}
\subsection{SVD}
For any matrix $A\in R^{m\times n}$, we have $A=U\Sigma V^T$, where $UU^T=VV^T=I$, and $\Sigma$ is the diagonal matrix $\Sigma=diag\{\sigma_i\}, \sigma_i\ge 0$

{\bf Discussion:} For matrix-vector multiplication, we could decompose it into rotation-scaling-rotation. $Ax=U\Sigma V^Tx=(U(\Sigma(V^Tx)))$, and we can interpret it from right to left.

\subsection{EVD}
For any real symmetric matrix, we could get the eigen value decomposition of it. $A=Q\Sigma Q^T$

\begin{definition}
	Positive Semi-Definite Matrix: $\forall v\in R^n, v^TAv\ge0$
\end{definition}
\begin{theorem}
	$A$ is PSD, iff. $\lambda_i\ge0, \forall i$.
\end{theorem} 
\begin{proof}
	The sufficient part: for any eigenvector $v$, we have $v^TAv=v^T\lambda_i v=\lambda_i\ge0$. For necessary part, for any vector $v$, we have $v^TAv=v^TQ\Sigma Q^Tv=\sum_i \lambda_i(v')_i^2\ge0$
\end{proof}
\begin{proposition}
	For PSD matrix, we have $A^{1/2}=Qdiag\{\sqrt{\lambda_i}\}Q^T$
\end{proposition}
\begin{proposition}
	$f(x) = \log\det(x)$, then the gradient is $x^{-1}$.
\end{proposition}
\begin{proof}
\begin{eqnarray}
f(x+\Delta x) &=& \log \det (x+\Delta x)=\log\det(x^{1/2}(I+x^{-1/2}\Delta xx^{-1/2})x^{1/2})\\
&=&\log \det(x) + \log\det(I+x^{-1/2}\Delta xx^{-1/2})=\log \det(x) +\sum_i \log(1+\lambda'_i)\\
&=&\log \det(x) +\sum_i \lambda'_i = \log \det(x) + Tr(x^{-1/2}\Delta xx^{-1/2}) \\
&=&\log \det(x) + Tr(x^{-1}\Delta x) 
\end{eqnarray}
Thus, the gradient is $x^{-1}$.
\end{proof}
{\bf Discussion: }How to calculate the second derivative? 
\begin{eqnarray}
	g(x+\Delta x)&=&(x+\Delta x)^{-1} = (x^{1/2}(I+x^{-1/2}\Delta x x^{-1/2})x^{1/2})^{-1}\\
	&=&x^{-1/2}(I+x^{-1/2}\Delta x x^{-1/2})^{-1}x^{-1/2}=x^{-1/2}(I-x^{-1/2}\Delta x x^{-1/2})x^{-1/2}\\
	&=&x^{-1} - x^{-1}\Delta x x^{-1}
\end{eqnarray}
\end{document}
